{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.4/dist-packages (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.4/dist-packages (from pandas) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.4/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.4/dist-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.4/dist-packages (from python-dateutil>=2->pandas) (1.10.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.4/dist-packages (0.72.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.4/dist-packages (from xgboost) (1.15.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.4/dist-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.4/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.4/dist-packages (from sklearn) (0.19.2)\n",
      "Requirement already satisfied: six==1.10 in /usr/local/lib/python3.4/dist-packages (1.10.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.4/dist-packages (2.2.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.4/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.4/dist-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.4/dist-packages (from matplotlib) (2018.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.4/dist-packages (from matplotlib) (1.10.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.4/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.4/dist-packages (from matplotlib) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.4/dist-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.4/dist-packages (0.9.0)\n",
      "Requirement already satisfied: patsy in /usr/local/lib/python3.4/dist-packages (from statsmodels) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.4 in /usr/local/lib/python3.4/dist-packages (from patsy->statsmodels) (1.15.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.4/dist-packages (from patsy->statsmodels) (1.10.0)\n",
      "Requirement already satisfied: keras==2.2 in /usr/local/lib/python3.4/dist-packages (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (1.15.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (1.10.0)\n",
      "Requirement already satisfied: keras-preprocessing==1.0.1 in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (2.8.0)\n",
      "Requirement already satisfied: keras-applications==1.0.2 in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (1.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.4/dist-packages (from keras==2.2) (3.13)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.4/dist-packages (1.9.0)\n",
      "Requirement already satisfied: tensorboard<1.10.0,>=1.9.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (1.9.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (0.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (1.10.0)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (39.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.4/dist-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.4/dist-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (2.6.11)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.4/dist-packages (from tensorboard<1.10.0,>=1.9.0->tensorflow) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install xgboost\n",
    "!pip3 install sklearn\n",
    "!pip3 install six==1.10\n",
    "!pip3 install matplotlib\n",
    "!pip3 install statsmodels\n",
    "!pip3 install keras==2.2\n",
    "!pip3 install tensorflow\n",
    "!pip3 install np_utils\n",
    "!pip3 install opencv-python\n",
    "!pip3 install optunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import random\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from xgboost import plot_tree\n",
    "\n",
    "#from graphviz import Digraph\n",
    "\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import cv2\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import optunity\n",
    "import optunity.metrics\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kyoto_1_score = pd.read_csv('dataset/data1_score.csv')\n",
    "kyoto_1_score.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kyoto_1_all = pd.read_csv('dataset/dataset1_15features.csv')\n",
    "kyoto_1_p1 = pd.read_csv('dataset/total01_1213.csv')\n",
    "kyoto_1_p2 = pd.read_csv('dataset/total01_0123~0124.csv')\n",
    "kyoto_1_p3 = pd.read_csv('dataset/total01_1122.csv')\n",
    "kyoto_week = pd.read_csv('dataset/Week1filter.csv')\n",
    "\n",
    "\n",
    "kyoto_1_all.rename(index=str, columns={\"Userid\": \"userid\"}, inplace=True)\n",
    "kyoto_1_all = kyoto_1_all.drop(['Score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k_temp = kyoto_1_all.drop(['userid'], axis=1)\n",
    "\n",
    "noise1 = kyoto_1_all[list(k_temp)].mul(np.random.normal(1.0, 0.5, size=len(kyoto_1_all)), axis=0)\n",
    "noise1 = noise1.join(kyoto_1_all['userid'].to_frame())\n",
    "\n",
    "noise2 = kyoto_1_all[list(k_temp)].mul(np.random.normal(1.0, 0.5, size=len(kyoto_1_all)), axis=0)\n",
    "noise2 = noise2.join(kyoto_1_all['userid'].to_frame())\n",
    "\n",
    "kyoto_1_all = pd.concat([kyoto_1_all, noise1])\n",
    "kyoto_1_all = pd.concat([kyoto_1_all, noise2])\n",
    "kyoto_1_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kyoto_x_all = pd.merge(kyoto_1_all, kyoto_1_score, on='userid')\n",
    "kyoto_x_1 = pd.merge(kyoto_1_p1, kyoto_1_score, on='userid')\n",
    "kyoto_x_2 = pd.merge(kyoto_1_p2, kyoto_1_score, on='userid')\n",
    "kyoto_x_3 = pd.merge(kyoto_1_p3, kyoto_1_score, on='userid')\n",
    "kyoto_week = pd.merge(kyoto_1_p3, kyoto_1_score, on='userid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kyoto_x_p = kyoto_x_1.append([kyoto_x_2, kyoto_x_3])\n",
    "kyoto_x_p.rename(index=str, columns={\"ADD BOOKMARK\": \"Add_BookmarkC\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"CLOSE\": \"closec\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"MARKERC\": \"markerc\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"MEMOC\": \"memoc\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"OPEN\": \"openc\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"PAGE_JUMP\": \"Page_JumpC\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"mobile\": \"mobilec\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"pc\": \"pcc\"}, inplace=True)\n",
    "kyoto_x_p.rename(index=str, columns={\"tablet\": \"tabletc\"}, inplace=True)\n",
    "#set(kyoto_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kyoto_1 = kyoto_x_all\n",
    "#kyoto_1 = kyoto_x_p\n",
    "#kyoto_1 = kyoto_week\n",
    "#kyoto_1 = pd.concat([kyoto_x_all, kyoto_x_p])\n",
    "\n",
    "\n",
    "kyoto_1.fillna(0.0, inplace=True)\n",
    "kyoto_1 = kyoto_1.drop(['userid'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Delete_Memo'], axis=1)\n",
    "\n",
    "'''\n",
    "kyoto_1 = kyoto_1.drop(['PREV'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['NEXT'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Readtime(seconds)'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['pcc'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['tabletc'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['LINK_CLICK'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['SEARCH_JUMP'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['memoc'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Add_BookmarkC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Delete_BookmarkC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Add_MemoC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Delete_MemoC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Change_MemoC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Add_MarkerC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['Delete_MarkerC'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['bookmarkc'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['closec'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['markerc'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['SEARCH'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['mobilec'], axis=1)\n",
    "kyoto_1 = kyoto_1.drop(['JUMPC'], axis=1)\n",
    "'''\n",
    "\n",
    "kyoto_1 = kyoto_1[kyoto_1['score']>0]\n",
    "#list(kyoto_1)[11]\n",
    "\n",
    "#kyoto_1 = kyoto_1.drop([list(kyoto_1)[14]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#outlier = 3\n",
    "#kyoto_1 = kyoto_1[np.abs(kyoto_1.pcc-kyoto_1.pcc.mean()) <= (outlier * kyoto_1.pcc.std())]\n",
    "#kyoto_1 = kyoto_1[np.abs(kyoto_1.tabletc-kyoto_1.tabletc.mean()) <= (outlier * kyoto_1.tabletc.std())]\n",
    "#kyoto_1 = kyoto_1 * 10\n",
    "#kyoto_1 = np.log(kyoto_1)\n",
    "#kyoto_1.fillna(0.0, inplace=True)\n",
    "#kyoto_1 = (kyoto_1 - kyoto_1.mean()) / (kyoto_1.max() - kyoto_1.min()) * 10\n",
    "#kyoto_1.fillna(0.0, inplace=True)\n",
    "\n",
    "#kyoto_1\n",
    "#kyoto_1.plot.box(sym='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "kyoto_y = kyoto_1['score']\n",
    "kyoto_x = kyoto_1.drop(['score'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kyoto_x * np.random.normal(1.0, sd, size=len(kyoto_x))\n",
    "#kyoto_x = pd.concat([kyoto_x, kyoto_x.mul(np.random.normal(1.0, 0.5, size=len(kyoto_x)), axis=0)])\n",
    "#kyoto_x = pd.concat([kyoto_x, kyoto_x.mul(np.random.normal(1.0, 0.5, size=len(kyoto_x)), axis=0)])\n",
    "#kyoto_y = pd.concat([kyoto_y, kyoto_x.mul(np.random.normal(1.0, 0.5, size=len(kyoto_y)), axis=0)])\n",
    "#kyoto_y = pd.concat([kyoto_y, kyoto_x.mul(np.random.normal(1.0, 0.5, size=len(kyoto_y)), axis=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBRegressor(n_estimators=20000, \n",
    "                    learning_rate=0.01, \n",
    "                    gamma=0.1, \n",
    "                    subsample=0.9,\n",
    "                    colsample_bytree=0.9, \n",
    "                    max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_modelfit(alg, X_train, y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        cvresult = xgb.cv(xgb_param, \n",
    "                          xgtrain, \n",
    "                          num_boost_round=alg.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds,\n",
    "                          metrics='mae', \n",
    "                          early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train, eval_metric='mae')\n",
    "        \n",
    "    return alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_modelfitting(x_train, y_train):\n",
    "    \"\"\"Computes MSE of an SVR with RBF kernel and optimized hyperparameters.\"\"\"\n",
    "    \n",
    "    # define objective function for tuning\n",
    "    @optunity.cross_validated(x=x_train, y=y_train, num_iter=2, num_folds=5)\n",
    "    def tune_cv(x_train, y_train, C, degree, coef0):\n",
    "        model = sklearn.svm.SVR(C=C, degree=degree, coef0=coef0, kernel='poly').fit(x_train, y_train)\n",
    "        predictions = model.predict(x_train)\n",
    "        return optunity.metrics.mse(y_train, predictions)\n",
    "\n",
    "    # optimize parameters\n",
    "    optimal_pars, _, _ = optunity.minimize(tune_cv, \n",
    "                                           150, \n",
    "                                           C=[1000, 20000], \n",
    "                                           degree=[2, 5], \n",
    "                                           coef0=[0, 1])\n",
    "    #print(\"optimal hyperparameters: \" + str(optimal_pars))\n",
    "\n",
    "    tuned_model = sklearn.svm.SVR(kernel='poly', **optimal_pars).fit(x_train, y_train)\n",
    "\n",
    "    return tuned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(2,),\n",
    "                   activation='relu',\n",
    "                   solver='adam',\n",
    "                   learning_rate='adaptive',\n",
    "                   max_iter=20000,\n",
    "                   learning_rate_init=0.01,\n",
    "                   alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(14, input_dim=14, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(GaussianNoise(0.1))\n",
    "    model.add(Dense(8, input_dim=8, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mse_xgb_list = []\n",
    "mse_pcx_list = []\n",
    "mse_ols_list = []\n",
    "mse_svr_list = []\n",
    "mse_mlp_list = []\n",
    "mse_pcr_list = []\n",
    "mse_regr_list = []\n",
    "mse_knn_list = []\n",
    "\n",
    "mape_xgb_list = []\n",
    "mape_pcx_list = []\n",
    "mape_regr_list = []\n",
    "mape_ols_list = []\n",
    "mape_svr_list = []\n",
    "mape_mlp_list = []\n",
    "mape_pcr_list = []\n",
    "mape_knn_list = []\n",
    "\n",
    "r2_xgb_list = []\n",
    "r2_xgb_list = []\n",
    "r2_pcx_list = []\n",
    "r2_regr_list = []\n",
    "r2_ols_list = []\n",
    "r2_svr_list = []\n",
    "r2_mlp_list = []\n",
    "r2_pcr_list = []\n",
    "r2_knn_list = []\n",
    "\n",
    "tl_knn_list = []\n",
    "\n",
    "for i in range(0, 50):\n",
    "    \n",
    "    print (i)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(kyoto_x, kyoto_y, test_size=0.3)\n",
    "    scale = StandardScaler()\n",
    "    X_train = scale.fit_transform(X_train)\n",
    "    X_test = scale.fit_transform(X_test)\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb_modelfit(xgb2, X_train, y_train)\n",
    "    #xgb2.fit(X_train, y_train)\n",
    "    xgr_train_y = xgb_model.predict(X_train)\n",
    "    xgr_pred_y = xgb_model.predict(X_test)\n",
    "    mse_xgb_list.append(sqrt(mean_squared_error(y_test, xgr_pred_y)))\n",
    "    mape_xgb_list.append(((y_test - xgr_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_xgb_list.append(r2_score(xgr_pred_y, y_test))\n",
    "    \n",
    "    # PCX\n",
    "    pca = PCA(n_components=9)# adjust yourself\n",
    "    pca.fit(X_train)\n",
    "    X_pca_train = pca.transform(X_train)\n",
    "    X_pca_test = pca.transform(X_test)    \n",
    "    #xgb2.fit(X_pca_train, y_train)\n",
    "    xgb_model = xgb_modelfit(xgb2, X_pca_train, y_train)\n",
    "    pcx_pred_y = xgb_model.predict(X_pca_test)\n",
    "    mse_pcx_list.append(sqrt(mean_squared_error(y_test, pcx_pred_y)))\n",
    "    mape_pcx_list.append(((y_test - pcx_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_pcx_list.append(r2_score(y_test, pcx_pred_y))\n",
    "    \n",
    "    # Random Forest Regressor\n",
    "    regr = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "    regr.fit(X_train, y_train)\n",
    "    regr_pred_y = regr.predict(X_test)\n",
    "    mse_regr_list.append(sqrt(mean_squared_error(y_test, regr_pred_y)))\n",
    "    mape_regr_list.append(((y_test - regr_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_regr_list.append(r2_score(y_test, regr_pred_y))\n",
    "    \n",
    "    # linear regression\n",
    "    ols_model = sm.OLS(endog=y_train, exog=X_train)\n",
    "    ols_model = ols_model.fit()\n",
    "    ols_pred_y = ols_model.predict(exog=X_test)\n",
    "    mse_ols_list.append(sqrt(mean_squared_error(y_test, ols_pred_y)))\n",
    "    mape_ols_list.append(((y_test - ols_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_ols_list.append(r2_score(y_test, ols_pred_y))\n",
    "    \n",
    "    # support vector regression\n",
    "    clf = SVR(C=1.0, epsilon=0.2)\n",
    "    #svr_model = svr_modelfitting(X_train, y_train)\n",
    "    svr_model = clf.fit(X_train, y_train) \n",
    "    svr_pred_y = svr_model.predict(X_test)\n",
    "    mse_svr_list.append(sqrt(mean_squared_error(y_test, svr_pred_y)))\n",
    "    mape_svr_list.append(((y_test - svr_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_svr_list.append(r2_score(y_test, svr_pred_y))\n",
    "    \n",
    "    # Multi-layer Perceptron regressor\n",
    "    mlp_model = mlp.fit(X_train, y_train)\n",
    "    mlp_pred_y = mlp_model.predict(X_test)\n",
    "    mse_mlp_list.append(sqrt(mean_squared_error(y_test, mlp_pred_y)))\n",
    "    mape_mlp_list.append(((y_test - mlp_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_mlp_list.append(r2_score(y_test, mlp_pred_y))\n",
    "    \n",
    "    # Princple Componment Regression\n",
    "    pca = PCA(n_components=9)# adjust yourself\n",
    "    pca.fit(X_train)\n",
    "    X_pca_train = pca.transform(X_train)\n",
    "    X_pca_test = pca.transform(X_test)\n",
    "    pcr_model = sm.OLS(endog=y_train, exog=X_pca_train)\n",
    "    pcr_model = pcr_model.fit()\n",
    "    pcr_pred_y = pcr_model.predict(exog=X_pca_test)\n",
    "    mse_pcr_list.append(sqrt(mean_squared_error(y_test, pcr_pred_y)))\n",
    "    mape_pcr_list.append(((y_test - pcr_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_pcr_list.append(r2_score(y_test, pcr_pred_y))\n",
    "    \n",
    "    # keras NN\n",
    "    knn = KerasRegressor(build_fn=baseline_model, epochs=5000, batch_size=3, verbose=0)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred_y = knn.predict(X_test)\n",
    "    mse_knn_list.append(sqrt(mean_squared_error(y_test, knn_pred_y)))\n",
    "    mape_knn_list.append(((y_test - knn_pred_y) / y_test).abs().mean()*100)\n",
    "    r2_knn_list.append(r2_score(y_test, knn_pred_y))\n",
    "    knn_pred_y = knn.predict(X_train)\n",
    "    tl_knn_list.append(sqrt(mean_squared_error(y_train, knn_pred_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(18, 8))\n",
    "ax1.set_title('RMSE')\n",
    "box_data = [mse_xgb_list,\n",
    "            mse_pcx_list, \n",
    "            mse_regr_list,\n",
    "            mse_svr_list, \n",
    "            mse_ols_list,\n",
    "            mse_pcr_list,\n",
    "            mse_mlp_list, \n",
    "            mse_knn_list]\n",
    "\n",
    "#ax1.set_yscale('log')\n",
    "ax1.boxplot(box_data)\n",
    "ax1.set_ylim([10, 100])\n",
    "ax1.set_xticklabels(['XGBoost',\n",
    "                     'PCXGBoox', \n",
    "                     'RFR', \n",
    "                     'SVR', \n",
    "                     'MLR',\n",
    "                     'PCR',\n",
    "                     'ANN', \n",
    "                     'Keras'], rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('MSE XGBoost:', '{:.2f}'.format(sum(mse_xgb_list) / len(mse_xgb_list)))\n",
    "print ('MSE PCXGBoost:', '{:.2f}'.format(sum(mse_pcx_list) / len(mse_pcx_list)))\n",
    "print ('MSE Random Forest Regression:', '{:.2f}'.format(sum(mse_regr_list) / len(mse_regr_list)))\n",
    "print ('MSE Support Vector Regression:', '{:.2f}'.format(sum(mse_svr_list) / len(mse_svr_list)))\n",
    "print ('MSE Artificial Neural Network:', '{:.2f}'.format(sum(mse_mlp_list) / len(mse_mlp_list)))\n",
    "print ('MSE Mutiple Linear Regression:', '{:.2f}'.format(sum(mse_ols_list) / len(mse_ols_list)))\n",
    "print ('MSE Principle Component Regression:', '{:.2f}'.format(sum(mse_pcr_list) / len(mse_pcr_list)))\n",
    "print ('MSE Keras Neural Network:', '{:.2f}'.format(sum(mse_knn_list) / len(mse_knn_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(18, 8))\n",
    "ax1.set_title('MAPE')\n",
    "box_data = [mape_xgb_list,\n",
    "            mape_pcx_list, \n",
    "            mape_regr_list,\n",
    "            mape_svr_list, \n",
    "            mape_mlp_list, \n",
    "            mape_ols_list, \n",
    "            mape_pcr_list,\n",
    "            mape_knn_list]\n",
    "\n",
    "#ax1.set_yscale('log')\n",
    "ax1.boxplot(box_data)\n",
    "ax1.set_ylim([0, 120])\n",
    "ax1.set_xticklabels(['XGBoost',\n",
    "                     'PCXGBoox', \n",
    "                     'RFR', \n",
    "                     'SVR', \n",
    "                     'ANN', \n",
    "                     'PCR',\n",
    "                     'Keras'], rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('MAPE XGBoost:', '{:.2f}'.format(sum(mape_xgb_list) / len(mape_xgb_list)))\n",
    "print ('MAPE PCXGBoost:', '{:.2f}'.format(sum(mape_pcx_list) / len(mape_pcx_list)))\n",
    "print ('MAPE Random Forest Regression:', '{:.2f}'.format(sum(mape_regr_list) / len(mape_regr_list)))\n",
    "print ('MAPE Support Vector Regression:', '{:.2f}'.format(sum(mape_svr_list) / len(mape_svr_list)))\n",
    "print ('MAPE Artificial Neural Network:', '{:.2f}'.format(sum(mape_mlp_list) / len(mape_mlp_list)))\n",
    "print ('MAPE Mutiple Linear Regression:', '{:.2f}'.format(sum(mape_ols_list) / len(mape_ols_list)))\n",
    "print ('MAPE Principle Component Regression:', '{:.2f}'.format(sum(mape_pcr_list) / len(mape_pcr_list)))\n",
    "print ('MAPE Keras Neural Network:', '{:.2f}'.format(sum(mape_knn_list) / len(mape_knn_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('R2 XGBoost:', '{:.2f}'.format(sum(r2_xgb_list) / len(r2_xgb_list)))\n",
    "print ('R2 PCXGBoost:', '{:.2f}'.format(sum(r2_pcx_list) / len(r2_pcx_list)))\n",
    "print ('R2 Random Forest Regression:', '{:.2f}'.format(sum(r2_regr_list) / len(r2_regr_list)))\n",
    "print ('R2 Support Vector Regression:', '{:.2f}'.format(sum(r2_svr_list) / len(r2_svr_list)))\n",
    "print ('R2 Artificial Neural Network:', '{:.2f}'.format(sum(r2_mlp_list) / len(r2_mlp_list)))\n",
    "print ('R2 Mutiple Linear Regression:', '{:.2f}'.format(sum(r2_ols_list) / len(r2_ols_list)))\n",
    "print ('R2 Principle Component Regression:', '{:.2f}'.format(sum(r2_pcr_list) / len(r2_pcr_list)))\n",
    "print ('R2 Keras Neural Network:', '{:.2f}'.format(sum(r2_knn_list) / len(r2_knn_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_frame()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
